---
layout: post
title: "Modeling Complex Interactions in Long Documents for Aspect-Based Sentiment Analysis"
published: true
---

<p align="center">
  <strong>Zehong Yan, Wynne Hsu, Mong Li Lee, David Bartram-Shaw</strong>
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2403.03170"> 
    <img src="https://img.shields.io/badge/arXiv-B31B1B?logo=arxiv&labelColor=grey" />
  </a> 
  <a href="https://github.com/YanZehong/dart"> 
    <img src="https://img.shields.io/badge/Model-181717?logo=github&labelColor=grey" />
  </a> 
  <a href="https://github.com/YanZehong/SocialNews"> 
    <img src="https://img.shields.io/badge/Data-4285F4?logo=googledocs&logoColor=white&labelColor=grey" />
  </a> 
</p>

<div class="img-div-any-width" markdown="0">
  <image src="/images/Sniffer/introcase.jpg"/>
</div>


<blockquote class='subtle'>
  WASSA, ACL Workshop, 2024 / <a href="https://arxiv.org/pdf/2403.03170">PDF</a> / <a href="https://yanzehong.github.io/Dart/">Project Page</a> / <a href="https://github.com/YanZehong/dart">Code</a> / <a href="https://github.com/YanZehong/SocialNews">Data</a>
</blockquote>


This work introduces a hierarchical Transformer-based architecture, DART, that encodes information at different level of granularities with attention aggregation mechanisms to learn the local and global aspect-specific document representations. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues.
<!--more-->




## Abstract
<p align="justify">
  The growing number of online articles and reviews necessitates innovative techniques for document-level aspect-based sentiment analysis. Capturing the context in which an aspect is mentioned is crucial. Existing models have focused on relatively short reviews and may fail to consider distant contextual information. This is especially so in longer documents where an aspect may be referred to in multiple ways across dispersed sentences. This work introduces a hierarchical Transformer-based architecture that encodes information at different level of granularities with attention aggregation mechanisms to learn the local and global aspect-specific document representations. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues. Experimental results show that the proposed architecture outperforms state-of-the-art methods for document-level aspect-based sentiment classification. We also demonstrate the potential applicability of our approach for long document trust prediction.
</p>


## Framework
<div class="img-div-any-width" markdown="0">
  <image src="/images/Sniffer/framework.jpg"/>
</div>

<p align="justify">
  Architecture of the proposed framework SNIFFER. For a given image-text pair, SNIFFER conducts a two-pronged analysis: (1) it checks the consistency of the image and text content (<em>internal checking</em>), and (2) it examines the relevance between the context of the retrieved image and the provided text (<em>external checking</em>). The outcomes of both these verification processes are then considered by S NIFFER to arrive at a final judgment and explanation.
</p>

## Multimodal Instrucion-Following Data

### Instruction Tuning Process

<div class="img-div-any-width" markdown="0">
  <image src="/images/Sniffer/process.jpg"/>
</div>

### Task-Specific Instruction Construction (with judgment and explanation)

<div class="img-div-any-width" markdown="0">
  <image src="/images/Sniffer/oocdata.jpg"/>
</div>

## Performance
<div class="img-div-any-width" markdown="0">
  <image src="/images/Sniffer/detection.jpg"/>
</div>

## BibTeX
If you found this work helpful for your research, please cite it as following:
```
@inproceedings{qi2023sniffer,
  author      = {Qi, Peng and Yan, Zehong and Hsu, Wynne and Lee, Mong Li},
  title       = {SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection},
  booktitle   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year        = {2024}
}
```


<footer class="footer">
  <p>
    This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a hred="https://pengqi.site/Sniffer/">Sniffer</a>. Thanks for the great work.
  </p>
</footer>
