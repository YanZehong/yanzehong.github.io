---
layout: post
title: "Modeling Complex Interactions in Long Documents for Aspect-Based Sentiment Analysis"
published: true
---

<p align="center">
  <strong>Zehong Yan<sup style="font-size: 70%;vertical-align: super;">1</sup>, Wynne Hsu<sup style="font-size: 70%;vertical-align: super;">1</sup>, Mong Li Lee<sup style="font-size: 70%;vertical-align: super;">1</sup>, David Roy Bartram-Shaw<sup style="font-size: 70%;vertical-align: super;">2</sup></strong>
  <br>
  <sup style="font-size: 70%;vertical-align: super;">1</sup>NUS Centre for Trusted Internet & Community, National University of Singapore
  <br>
  <sup style="font-size: 70%;vertical-align: super;">2</sup>Edelman Data & Intelligence
</p>

<p align="center">
  <a href="https://aclanthology.org/2024.wassa-1.3/"> 
    <img src="https://img.shields.io/badge/paper-B31B1B?logo=arXiv&labelColor=grey" />
  </a> 
  <a href="https://github.com/YanZehong/dart"> 
    <img src="https://img.shields.io/badge/Model-181717?logo=github&labelColor=grey" />
  </a> 
  <a href="https://github.com/YanZehong/SocialNews"> 
    <img src="https://img.shields.io/badge/Data-4285F4?logo=googledocs&logoColor=white&labelColor=grey" />
  </a> 
</p>

<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/intro.jpeg" width="310" height="350" />
</div>


<blockquote class='subtle'>
  WASSA Workshop, ACL, 2024 / <a href="https://aclanthology.org/2024.wassa-1.3/">PDF</a> / <a href="https://yanzehong.github.io/dart/">Project Page</a> / <a href="https://github.com/YanZehong/dart">Code</a> / <a href="https://github.com/YanZehong/SocialNews">Data</a>
</blockquote>


We introduce DART, a hierarchical transformer-based framework for document-level aspect-based sentiment analysis.
DART handles the complexities of longer text through its  global context interaction and two-level aspect aggregation blocks, which enhance the model's ability to recognize and amplify aspect-specific content  across long-range dependencies. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues.
<!--more-->




## Abstract
<p align="justify">
  The growing number of online articles and reviews necessitates innovative techniques for document-level aspect-based sentiment analysis. Capturing the context in which an aspect is mentioned is crucial. Existing models have focused on relatively short reviews and may fail to consider distant contextual information. This is especially so in longer documents where an aspect may be referred to in multiple ways across dispersed sentences. This work introduces a hierarchical Transformer-based architecture that encodes information at different level of granularities with attention aggregation mechanisms to learn the local and global aspect-specific document representations. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues. Experimental results show that the proposed architecture outperforms state-of-the-art methods for document-level aspect-based sentiment classification. We also demonstrate the potential applicability of our approach for long document trust prediction.
</p>

## Introduction
- Sentence-level aspect-based sentiment analysis fail to consider the aspect context, which can often be inferred from preceding or succeeding sentences or paragraphs.  
- Large language models (LLMs), such as GPT4, although powerful, still exhibit limited capabilities in modeling unknown aspects and get lost in long contexts.  
- We design a Transformer-based architecture, called DART, to capture dependency among sentences in long documents and learn aspect-specific document representations.


## Framework
<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/framework.jpeg" width="500" height="570"/>
</div>

The proposed DART framework consists of four key modules: 

- **Sentence Encoding Block** transforms the document into individual sentences and generate representations for sentence-aspect combination.

- **Global Context Interaction Block** models interactions among sentences and generate context-aware representations that capture aspect-specific information across long-range dependencies.

- **Aspect Aggregation Block** performs local and global attentive pooling to obtain the document representation.

- **Sentiment Classification Block** With the document representation obtained, this block leverages a two-layered Multilayer Perceptron (MLP) to predict the sentiment for  the aspect.


## Dataset 
_**Curate two datasets for aspect-based sentiment analysis in long documents: SocialNews and TrustData**_

<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/dataset.jpeg"/>
</div>

## Comparative Study

### Performance in TripAdvisor and BeerAdvocate
<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/result-2.png" width="350" height="200"/>
</div>
<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/result-3.png" width="400" height="300"/>
</div>
</p>


### Performance in SocialNews
<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/result-1.png"/>
</div>

<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/result.jpeg"/>
</div>


>[!IMPORTANT]
> - DART surpasses others in overall accuracy and excels in five key aspects, particularly in the digital-online and health aspects.
> - DART shows superior performance as document length increases.

## Application to Trust and Polarity Prediction

<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/result-5.png" width="500" height="550"/>
</div>
    
>[!IMPORTANT]
>DART gives the best performance in three key aspects and is competitive with GPT4-zeroshot for the Dependability aspect, due to the fewer number of documents with this aspect.


## Ablation

<div class="img-div-any-width" markdown="0">
  <image src="/images/dart/result-4.png" width="500" height="200"/>
</div>


## BibTeX
If you found this work helpful for your research, please cite it as following:
```
@inproceedings{yan-etal-2024-modeling,
    title = "Modeling Complex Interactions in Long Documents for Aspect-Based Sentiment Analysis",
    author = "Yan, Zehong and Hsu, Wynne and Lee, Mong-Li and Bartram-Shaw, David",
    booktitle = "Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wassa-1.3",
    pages = "23--34",
}
```


<footer class="footer">
  <p>
    This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a hred="https://pengqi.site/Sniffer/">Sniffer</a>. Thanks for the great work.
  </p>
</footer>
