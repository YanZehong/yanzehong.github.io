---
layout: post
title: "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection"
published: true
---

<p align="center">
  <strong>Zehong Yan<sup style="font-size: 70%;vertical-align: super;">1</sup>, Peng Qi<sup style="font-size: 70%;vertical-align: super;">1</sup>, Wynne Hsu<sup style="font-size: 70%;vertical-align: super;">1</sup>, Mong Li Lee<sup style="font-size: 70%;vertical-align: super;">1</sup></strong>
  <br>
  <sup style="font-size: 70%;vertical-align: super;">1</sup>NUS Centre for Trusted Internet & Community, National University of Singapore
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2501.14728"> 
    <img src="https://img.shields.io/badge/paper-B31B1B?logo=arXiv&labelColor=grey" />
  </a> 
  <a href="https://yanzehong.github.io/" target="_blank"><img alt="Homepage"
    src="https://img.shields.io/badge/TRUST--VL-Homepage-7289da?logo=github&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/YanZehong/TRUST-VL"> 
    <img src="https://img.shields.io/badge/Code-181717?logo=github&labelColor=grey" />
  </a> 
  <a href="https://huggingface.co" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-TRUST--VL-ffc107?color=ffc107&logoColor=white"/></a>
  <a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE"><img alt="License"
    src="https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg"/></a>
</p>

<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/trust-vl-logo.png"/>
</div>
</p>


<blockquote class='subtle'>
  arXiv, 2025 / <a href="https://arxiv.org/pdf/2501.14728">PDF</a> / <a href="https://yanzehong.github.io/pollutino/">Project Page</a> / <a href="https://github.com/YanZehong/GenAI-Evidence-Pollution">Code</a> / <a href="https://github.com/YanZehong/GenAI-Evidence-Pollution/tree/main/data">Data</a>
</blockquote>


<p align="justify">
We observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. </p>
<!--more-->

