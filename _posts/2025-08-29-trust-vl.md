---
layout: post
title: "TRUST-VL: A Unified and Explainable Vision–Language Model for General Multimodal Misinformation Detection"
published: true
---

<p align="center">
  <strong>Zehong Yan, Peng Qi, Wynne Hsu and Mong Li Lee</strong>
  <br>
  NUS Centre for Trusted Internet & Community, National University of Singapore
</p>

<p align="center">
  <a href="https://github.com/YanZehong/TRUST-VL"> 
    <img src="https://img.shields.io/badge/Code-181717?logo=github&labelColor=grey" />
  </a> 
</p>


<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/abilities.png"/>
</div>
</p>


<blockquote class='subtle'>
  arXiv, 2025 / <a href="">PDF</a> / <a href="https://yanzehong.github.io/trust-vl/">Project Page</a> / <a href="https://github.com/YanZehong/TRUST-VL">Code</a> / <a href="https://github.com/YanZehong/TRUST-VL/tree/main/data">Data</a>
</blockquote>


<p align="justify">
A unified and explainable VLM for multimodal misinformation detection across textual, visual, and cross-modal distortions, powered by a novel Question-Aware Visual Amplifier (QAVA) and a 198K instruction dataset (TRUST-Instruct).
</p>
<!--more-->


<!-- Local styles for section blocks -->
<style>
.section-grey  { background: #f6f8fa; padding: 1.25rem 1.5rem; border-radius: 12px; }
.section-white { background: #ffffff; padding: 1.25rem 1.5rem; border-radius: 12px; }
.section-spacer { height: 12px; }
</style>


<div class="section-grey" markdown="1">
  
## Abstract

<p align="justify">
Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. We introduce <b>TRUST-VL</b>, a unified and explainable vision–language model for general multimodal misinformation detection. TRUST-VL incorporates a novel <b>Question-Aware Visual Amplifier (QAVA)</b> module, designed to extract task-specific visual features. To support training, we also construct <b>TRUST-Instruct</b>, a large-scale instruction dataset containing <b>198K</b> samples with structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate state-of-the-art performance, with strong generalization and interpretability.
</p>

</div>

## Highlights
- We propose **TRUST-VL**, a unified and explainable vision-language model for general multimodal misinformation detection. It integrates a novel **Question-Aware Visual Amplifier (QAVA)** module to extract task-specific visual features and support reasoning across misinformation detection tasks.
- We construct **TRUST-Instruct**, a large-scale instruction dataset of _198K_ samples with structured reasoning chains aligned with human fact-checking workflows, enabling effective joint training across diverse distortion types.
- Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, with superior generalization and interpretability compared to existing detectors and general VLMs.


## Method Overview

<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/framework.png"/>
</div>
</p>


<p align="justify">
Given an image–text claim, TRUST-VL retrieves external evidence and performs structured, step-wise reasoning. Text, evidence, and targeted questions are encoded by a textual encoder, while the image is processed by a visual encoder with a general projector and the <b>QAVA</b> module. The resulting tokens are fed into an LLM to yield a final judgment and explanation.
</p>

### Question-Aware Visual Amplifier (QAVA)
<p align="justify">
QAVA inserts a small set of learnable, question-conditioned tokens. Via self-attention (on the question) and cross-attention (to image features), these tokens extract precise, task-relevant cues (e.g., subtle facial edits) without degrading performance on other distortion types. The enhanced features act as soft visual prompts to guide the LLM’s reasoning.
</p>


## TRUST-Instruct

<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/trust-instruct.png"/>
</div>
</p>
    
<p align="justify">
We construct TRUST-Instruct by prompting a strong VLM with a structured reasoning template and then verifying outputs for label consistency. Each reasoning chain begins with shared steps (text analysis, image description) and branches into task-specific checks for textual, visual, and cross-modal distortions.
</p>

## Performance Study

### Baselines & Setup
<p align="justify">
We compare TRUST-VL to general-purpose VLMs (BLIP-2, InstructBLIP, LLaVA, LLaVA-NeXT, xGen-MM, Qwen2-VL, GPT-4o, o1) and misinformation detectors (MMD-Agent, SNIFFER, LRQ-FACT). We fine-tune LLaVA-1.5 (Vicuna-13B; CLIP ViT-L/14) in three stages: connector pre-alignment, instruction following, and misinformation-specific reasoning on TRUST-Instruct-198K.
</p>

### Main Results
<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/results.png"/>
</div>
</p>

<p align="justify">
TRUST-VL achieves state-of-the-art average accuracy across diverse datasets, with particularly large gains on fine-grained visual manipulation and robust generalization to out-of-domain benchmarks.
</p>


<p align="justify">
Ablations confirm the importance of structured reasoning, shared reasoning steps, and QAVA. Varying the number of QAVA tokens shows that <em>32 tokens</em> provide a strong balance of accuracy and efficiency across datasets.
</p>

## Case Studies
<p align="justify">
Representative examples across textual, visual, and cross-modal distortions show how TRUST-VL pinpoints the deceptive elements and justifies its verdict with interpretable, step-wise explanations.
</p>


<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/case.png"/>
</div>
</p>

## BibTeX
If you find this work helpful, please cite:


```
@article{yan2025trustvl,
  title={TRUST-VL: A Unified and Explainable Vision–Language Model for General Multimodal Misinformation Detection},
  author={Yan, Zehong and Qi, Peng and Hsu, Wynne and Lee, Mong Li},
  publisher={To appear},
  year={2025}
}
```


<footer class="footer">
  <p>
    This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a hred="https://pengqi.site/Sniffer/">Sniffer</a>. We thank the LLaVA team for giving us access to their models, and open-source projects. 
  </p>

  <p>
    Usage and License Notices: The data, code and checkpoint is intended and licensed for research use only. 
  </p>
</footer>
