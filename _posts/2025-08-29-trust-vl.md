---
layout: post
title: "TRUST-VL: A Unified and Explainable Vision–Language Model for General Multimodal Misinformation Detection"
published: true
---

<p align="center">
  <strong>Zehong Yan, Peng Qi, Wynne Hsu and Mong Li Lee</strong>
  <br>
  NUS Centre for Trusted Internet & Community, National University of Singapore
</p>

<p align="center">
  <a href="https://github.com/YanZehong/TRUST-VL"> 
    <img src="https://img.shields.io/badge/Code-181717?logo=github&labelColor=grey" />
  </a> 
</p>


<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/abilities.png"/>
</div>
</p>


<blockquote class='subtle'>
  arXiv, 2025 / <a href="">PDF</a> / <a href="https://yanzehong.github.io/trust-vl/">Project Page</a> / <a href="https://github.com/YanZehong/TRUST-VL">Code</a> / <a href="https://github.com/YanZehong/TRUST-VL/tree/main/data">Data</a>
</blockquote>


<p align="justify">
A unified and explainable VLM for multimodal misinformation detection across textual, visual, and cross-modal distortions, powered by a novel Question-Aware Visual Amplifier (QAVA) and a 198K instruction dataset (TRUST-Instruct).
</p>
<!--more-->


<!-- Local styles for section blocks -->
<style>
.section-grey  { background: #f6f8fa; padding: 1.25rem 1.5rem; border-radius: 12px; }
.section-spacer { height: 12px; }
</style>


  
## Abstract
<div class="section-grey" markdown="1">
<p align="justify">
Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. We introduce <b>TRUST-VL</b>, a unified and explainable vision–language model for general multimodal misinformation detection. TRUST-VL incorporates a novel <b>Question-Aware Visual Amplifier (QAVA)</b> module, designed to extract task-specific visual features. To support training, we also construct <b>TRUST-Instruct</b>, a large-scale instruction dataset containing <b>198K</b> samples with structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate state-of-the-art performance, with strong generalization and interpretability.
</p>

</div>

## Highlights
- We propose **TRUST-VL**, a unified and explainable vision-language model for general multimodal misinformation detection. It integrates a novel **Question-Aware Visual Amplifier (QAVA)** module to extract task-specific visual features and support reasoning across misinformation detection tasks.
- We construct **TRUST-Instruct**, a large-scale instruction dataset of _198K_ samples with structured reasoning chains aligned with human fact-checking workflows, enabling effective joint training across diverse distortion types.
- Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, with superior generalization and interpretability compared to existing detectors and general VLMs.


## Method Overview
<div class="section-grey" markdown="1">
<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/framework.png"/>
</div>
</p>


<p align="justify">
Given an image–text claim, TRUST-VL retrieves external evidence and performs structured, step-wise reasoning. Text, evidence, and targeted questions are encoded by a textual encoder, while the image is processed by a visual encoder with a general projector and the <b>QAVA</b> module. The resulting tokens are fed into an LLM to yield a final judgment and explanation.
</p>


### Question-Aware Visual Amplifier (QAVA)
<p align="justify">
QAVA inserts a small set of learnable, question-conditioned tokens. Via self-attention (on the question) and cross-attention (to image features), these tokens extract precise, task-relevant cues (e.g., subtle facial edits) without degrading performance on other distortion types. The enhanced features act as soft visual prompts to guide the LLM’s reasoning.
</p>


### TRUST-Instruct

<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/trust-instruct.png"/>
</div>
</p>
    
<p align="justify">
We construct TRUST-Instruct by prompting a strong VLM with a structured reasoning template and then verifying outputs for label consistency. Each reasoning chain begins with shared steps (text analysis, image description) and branches into task-specific checks for textual, visual, and cross-modal distortions.
</p>

</div>

  
## Performance Study

  
### Main Results
<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/results.png"/>
</div>
</p>

<p align="justify">
TRUST-VL achieves state-of-the-art average accuracy across diverse datasets, with particularly large gains on fine-grained visual manipulation and robust generalization to out-of-domain benchmarks.
</p>


## Case Studies
<div class="section-grey" markdown="1">
<p align="justify">
Representative examples across textual, visual, and cross-modal distortions show how TRUST-VL pinpoints the deceptive elements and justifies its verdict with interpretable, step-wise explanations.
</p>


<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/case.png"/>
</div>
</p>

</div>

## BibTeX
If you find this work helpful, please cite:


```
@article{yan2025trustvl,
  title={TRUST-VL: A Unified and Explainable Vision–Language Model for General Multimodal Misinformation Detection},
  author={Yan, Zehong and Qi, Peng and Hsu, Wynne and Lee, Mong Li},
  publisher={To appear},
  year={2025}
}
```

---

## Acknowledgement
<footer class="footer" style="text-align: left;">
  <p>
    This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>. We thank the <a href="https://github.com/haotian-liu/LLaVA">LLaVA</a> team for their open-source projects. 
  </p>

  <p>
    Usage and License Notices: The data, code and checkpoint is intended and licensed for research use only. 
  </p>
</footer>
