---
layout: post
title: "TRUST-VL: A Unified and Explainable Vision–Language Model for General Multimodal Misinformation Detection"
published: true
---

<p align="center">
  <strong>
    <a href="https://yanzehong.github.io/">Zehong Yan</a>, <a href="https://pengqi.website/">Peng Qi</a>, <a href="https://www.comp.nus.edu.sg/~whsu/">Wynne Hsu</a>, <a href="https://www.comp.nus.edu.sg/~leeml/">Mong Li Lee</a>
  </strong>
  <br>
  <b style="color:#5280BF; font-weight:normal">▶ </b>National University of Singapore
</p>


<div align="center" style="line-height: 1;">
  <a href="https://arxiv.org/abs/2509.04448"> 
    <img alt="arXiv" src="https://img.shields.io/badge/arXiv-Paper-B31B1B?logo=arXiv&labelColor=grey"/></a> 
  <a href="https://yanzehong.github.io/trust-vl/" target="_blank"><img alt="Homepage"
    src="https://img.shields.io/badge/TRUST--VL-Homepage-7289da?logo=googlegemini&logoColor=white&color=886FBF"/></a>
  <a href="https://github.com/YanZehong/TRUST-VL" target="_blank"><img alt="Homepage"
    src="https://img.shields.io/badge/Code-Data-7289da?logo=github&logoColor=white&color=7289da"/></a>
  <a href="https://huggingface.co/NUSryan/TRUST-VL-13b-task" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-TRUST--VL-ffc107?color=FFD21E&logoColor=white"/></a>
  <a href="https://huggingface.co/datasets/NUSryan/TRUST-Instruct" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-TRUST--Instruct-ffc107?color=ffc107&logoColor=white"/></a>
</div>



<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/abilities.png"/>
</div>
</p>


<blockquote class='subtle'>
  EMNLP, 2025 / <a href="https://arxiv.org/abs/2509.04448">PDF</a> / <a href="https://yanzehong.github.io/trust-vl/">Project Page</a> / <a href="https://github.com/YanZehong/TRUST-VL">Code</a> / <a href="https://github.com/YanZehong/TRUST-VL/data">Data</a>
</blockquote>


<p align="justify">
We present a unified and explainable vision–language model for various multimodal misinformation detection tasks across textual, visual, and cross-modal distortions, enhanced by the <i>Question-Aware Visual Amplifier (QAVA)</i> and supported by the large-scale <i>TRUST-Instruct</i> dataset of 198K reasoning samples.
</p>
<!--more-->


<!-- Local styles for section blocks -->
<style>
.section-grey  { background: #f6f8fa; padding: 1.25rem 1.5rem; border-radius: 12px; }
.section-spacer { height: 12px; }
</style>


---

## Abstract
<div class="section-grey" markdown="1">
<p align="justify">
Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model’s ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.
</p>

</div>

## Highlights
- We propose **TRUST-VL**, a unified and explainable vision-language model for general multimodal misinformation detection. It integrates a novel **Question-Aware Visual Amplifier (QAVA)** module to extract task-specific visual features and support reasoning across misinformation detection tasks.
- We construct **TRUST-Instruct**, a large-scale instruction dataset of _198K_ samples with structured reasoning chains aligned with human fact-checking workflows, enabling effective joint training across diverse distortion types.
- Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, with superior generalization and interpretability compared to existing detectors and general VLMs.


## Method Overview
<div class="section-grey" markdown="1">
<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/framework.png"/>
</div>
</p>


<p align="justify">
Given an image–text claim, TRUST-VL retrieves external evidence and performs structured, step-wise reasoning. Text, evidence, and targeted questions are encoded by a textual encoder, while the image is processed by a visual encoder with a general projector and the QAVA module. The resulting tokens are fed into an LLM to yield a final judgment and explanation.
</p>


### Question-Aware Visual Amplifier (QAVA)
<p align="justify">
QAVA inserts a small set of learnable, question-conditioned tokens. Via self-attention (on the question) and cross-attention (to image features), these tokens extract precise, task-relevant cues (e.g., subtle facial edits) without degrading performance on other distortion types. The enhanced features act as soft visual prompts to guide the LLM’s reasoning.
</p>


### TRUST-Instruct

<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/trust-instruct.png"/>
</div>
</p>
    
<p align="justify">
We construct TRUST-Instruct by prompting a strong VLM with a structured reasoning template and then verifying outputs for label consistency. Each reasoning chain begins with shared steps (text analysis, image description) and branches into task-specific checks for textual, visual, and cross-modal distortions.
</p>

</div>

  
## Performance Study

  
### Main Results
<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/results.png"/>
</div>
</p>

<p align="justify">
TRUST-VL achieves state-of-the-art average accuracy across diverse datasets, with particularly large gains on fine-grained visual manipulation and robust generalization to out-of-domain benchmarks.
</p>


## Case Studies
<div class="section-grey" markdown="1">
<p align="justify">
Representative examples across textual, visual, and cross-modal distortions show how TRUST-VL pinpoints the deceptive elements and justifies its verdict with interpretable, step-wise explanations.
</p>


<p align="center">
<div class="img-div-any-width" markdown="0">
  <image src="/images/TRUST-VL/case.png"/>
</div>
</p>

</div>

## BibTeX
If you find this work helpful, please cite:

```bibtex
@article{yan2025trustvl,
  title={TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection},
  author={Yan, Zehong and Qi, Peng and Hsu, Wynne and Lee, Mong Li},
  journal={arXiv preprint arXiv:2509.04448},
  year={2025}
}
```


## Acknowledgement
<footer class="footer" style="text-align: left;">
  <p>
    This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>. We thank the <a href="https://github.com/haotian-liu/LLaVA">LLaVA</a> team for their open-source projects and models. 
  </p>

  <p>
    Usage and License Notices: The data, code and checkpoint is intended and licensed for research use only. 
  </p>
</footer>
