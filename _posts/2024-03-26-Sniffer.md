---
layout: post
title: SNIFFER Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection
published: true
---
```
<div class="img-div-any-width" markdown="0">
  <image src="/images/Sniffer/introcase.jpg"/>
</div>
```
{: .center-image}
![]({{site.baseurl}}/images/Sniffer/introcase.jpg)

<blockquote class='subtle'>
  <strong>Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee</strong>
  CVPR, 2024
</blockquote>

Focusing on the innovative research perspective of explainable out-of-context misinformation detection, this paper proposes a new multimodal large language model, SNIFFER, designed to offer both accurate detection and persuasive explanations simultaneously. Enhanced by two-stage instruction tuning and retrieval-enhancement techniques, SNIFFER effectively models both internal image-text inconsistency and external claim-evidence relationships.
<!--more-->

## Abstract
```
Misinformation is a prevalent societal issue due to its potential high risks. Out-Of-Context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.
```

## BibTeX
If you found this work helpful for your research, please cite it as following:
```
@inproceedings{qi2023sniffer,
  author      = {Qi, Peng and Yan, Zehong and Hsu, Wynne and Lee, Mong Li},
  title       = {SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection},
  booktitle   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year        = {2024}
}
```


<footer class="footer">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a hred="https://pengqi.site/Sniffer/">Sniffer</a>. Thanks for the great work.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
